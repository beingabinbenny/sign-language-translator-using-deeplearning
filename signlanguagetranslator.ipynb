{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bke7s0Bo-182",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c2a11d7-8317-4e6e-f6a5-0dfdd53d0e63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCRdEwScfLgK"
      },
      "outputs": [],
      "source": [
        "!python -m venv myenv  # Create a virtual environment\n",
        "%cd myenv/bin\n",
        "!source activate\n",
        "%cd ../../  # Activate the virtual environment\n",
        "!pip install yolov5\n",
        "!pip list\n",
        "!git clone https://github.com/ultralytics/yolov5.git\n",
        "%cd yolov5\n",
        "!pip install -U -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VVxuB3nGDv8"
      },
      "outputs": [],
      "source": [
        "!python /content/myenv/bin/yolov5/train.py --img-size 640 --batch 16 --epochs 300 --data /content/drive/MyDrive/AmericanSignLanguageLetters.v1-v1.yolov5pytorch/custom.yaml --cfg /content/myenv/bin/yolov5/models/yolov5x.yaml --weights /content/drive/MyDrive/yolov5x.pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INqdFZwuvgil"
      },
      "outputs": [],
      "source": [
        "!python /content/myenv/bin/yolov5/detect.py --source /content/drive/MyDrive/AmericanSignLanguageLetters.v1-v1.yolov5pytorch/test/images --weights /content/myenv/bin/yolov5/runs/train/exp/weights/best.pt --img-size 640 --conf 0.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3GEhhWrw7sz"
      },
      "outputs": [],
      "source": [
        "!python /content/myenv/bin/yolov5/detect.py --source /content/drive/MyDrive/AmericanSignLanguageLetters.v1-v1.yolov5pytorch/test/images --weights /content/drive/MyDrive/AmericanSignLanguageLetters.v1-v1.yolov5pytorch/best.pt --img-size 640 --conf 0.4 --save-txt --save-conf --project /content/drive/MyDrive/AmericanSignLanguageLetters.v1-v1.yolov5pytorch --name test_results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/myenv/bin/yolov5/val.py --weights /content/300epochYolov5x.pt --data /content/drive/MyDrive/AmericanSignLanguageLetters.v1-v1.yolov5pytorch/custom.yaml --img 640 --conf 0.1 --iou 0.65\n"
      ],
      "metadata": {
        "id": "jIQvl9HAjYgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "code to generate confusion matrix\n"
      ],
      "metadata": {
        "id": "O1lG2v2u-V3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Enable inline plotting\n",
        "%matplotlib inline\n",
        "\n",
        "# Assuming 'ground_truth_labels' and 'predicted_labels' are lists of labels for each image\n",
        "# Example: ground_truth_labels = [0, 1, 2, 0, 1, 2] and predicted_labels = [0, 1, 2, 0, 2, 1]\n",
        "# Note: Replace the example lists with your actual ground truth and predicted labels\n",
        "\n",
        "ground_truth_labels = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
        "predicted_labels = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
        "\n",
        "# Compute confusion matrix\n",
        "conf_matrix = confusion_matrix(ground_truth_labels, predicted_labels)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.colorbar()\n",
        "\n",
        "# Add labels to the plot\n",
        "num_classes = len(np.unique(ground_truth_labels))\n",
        "plt.xticks(np.arange(num_classes), np.arange(num_classes))\n",
        "plt.yticks(np.arange(num_classes), np.arange(num_classes))\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ajB9odVIIhGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "import os\n",
        "# Set the path to the folder containing the images\n",
        "images_folder = '/content/drive/MyDrive/AmericanSignLanguageLetters.v1-v1.yolov5pytorch/test_results2'\n",
        "\n",
        "# Get the list of images in the folder\n",
        "image_files = [f for f in os.listdir(images_folder) if f.endswith(('.jpg', '.png'))]\n",
        "\n",
        "# Display each image\n",
        "for image_file in image_files:\n",
        "    image_path = os.path.join(images_folder, image_file)\n",
        "    display(Image(filename=image_path))\n"
      ],
      "metadata": {
        "id": "Z0dWYYswvD6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFB8VkiY4J55"
      },
      "outputs": [],
      "source": [
        "# Generate a dictionary mapping class IDs to letter names (A to Z)\n",
        "class_to_letter = {i: chr(65 + i) for i in range(26)}\n",
        "print(class_to_letter)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-colab"
      ],
      "metadata": {
        "id": "AqRbyavXHQbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "code to run the terminal line"
      ],
      "metadata": {
        "id": "1LlVNjArdgwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "\n",
        "# Define the terminal command\n",
        "terminal_command = '!python detect.py --source /content/drive/MyDrive/AmericanSignLanguageLetters.v1-v1.yolov5pytorch/test/images --weights /content/yolov5/runs/train/exp2/weights/best.pt --img-size 640 --conf 0.4 --save-txt --save-conf --project /content/drive/MyDrive/AmericanSignLanguageLetters.v1-v1.yolov5pytorch --name test_results'\n",
        "\n",
        "# Run the terminal command\n",
        "try:\n",
        "    subprocess.run(terminal_command, shell=True, check=True)\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(f\"Error: {e}\")\n"
      ],
      "metadata": {
        "id": "6xelBGhWcyGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "code to access camera using collab"
      ],
      "metadata": {
        "id": "OIQgLHE7djys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, HTML, Javascript\n",
        "from google.colab import output\n",
        "\n",
        "\n",
        "# JavaScript code to enable the camera and capture photo\n",
        "javascript_code = \"\"\"\n",
        "    <video autoplay></video>\n",
        "    <button>Take Photo</button>\n",
        "    <canvas></canvas>\n",
        "    <script>\n",
        "      const video = document.querySelector('video');\n",
        "      const canvas = document.querySelector('canvas');\n",
        "      const button = document.querySelector('button');\n",
        "\n",
        "      navigator.mediaDevices.getUserMedia({ video: true })\n",
        "        .then((stream) => {\n",
        "          video.srcObject = stream;\n",
        "        });\n",
        "\n",
        "      button.onclick = () => {\n",
        "        canvas.width = video.videoWidth;\n",
        "        canvas.height = video.videoHeight;\n",
        "        canvas.getContext('2d').drawImage(video, 0, 0, canvas.width, canvas.height);\n",
        "        const imgData = canvas.toDataURL('image/jpeg');\n",
        "        google.colab.kernel.invokeFunction('notebook.capture', [imgData], {});\n",
        "      };\n",
        "    </script>\n",
        "\"\"\"\n",
        "\n",
        "# Display the JavaScript code\n",
        "display(HTML(javascript_code))\n",
        "\n",
        "# Function to capture the image data\n",
        "def capture(img_data):\n",
        "    img_data = img_data[0]\n",
        "    display(HTML(f'<img src=\"{img_data}\" style=\"width:50%;\"/>'))\n",
        "\n",
        "# Register the function to capture the image data\n",
        "output.register_callback('notebook.capture', capture)\n"
      ],
      "metadata": {
        "id": "ba7pOtNZRbTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uEveD8t8Vs2Y"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}